{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import altair as alt\n",
    "alt.renderers.enable(\"notebook\")\n",
    "\n",
    "# Code for hiding seaborn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I appended a few extra autonomous car articles into the tech folder as well as into this file from the exercise\n",
    "df_path = 'news_dataset.csv'\n",
    "df = pd.read_csv(df_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>Complete_Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "      <td>001.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "      <td>002.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "      <td>003.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "      <td>004.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "      <td>005.txt-business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File_Name                                            Content  Category  \\\n",
       "0   001.txt  Ad sales boost Time Warner profit\\n\\nQuarterly...  business   \n",
       "1   002.txt  Dollar gains on Greenspan speech\\n\\nThe dollar...  business   \n",
       "2   003.txt  Yukos unit buyer faces loan claim\\n\\nThe owner...  business   \n",
       "3   004.txt  High fuel prices hit BA's profits\\n\\nBritish A...  business   \n",
       "4   005.txt  Pernod takeover talk lifts Domecq\\n\\nShares in...  business   \n",
       "\n",
       "  Complete_Filename  \n",
       "0  001.txt-business  \n",
       "1  002.txt-business  \n",
       "2  003.txt-business  \n",
       "3  004.txt-business  \n",
       "4  005.txt-business  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure it is formtated right...ish\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2228\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### begin parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\r and \\n\n",
    "df['Content_Parsed_1'] = df['Content'].str.replace(\"\\r\", \" \")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" when quoting text\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing the text\n",
    "df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation \n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
    "\n",
    "for punct_sign in punctuation_signs:\n",
    "    df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/helgy8/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/helgy8/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading punkt and wordnet from NLTK\n",
    "nltk.download('punkt')\n",
    "print(\"------------------------------------------------------------\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the lemmatizer into an object\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for loop\n",
    "nrows = len(df) # counter\n",
    "lemmatized_text_list = [] # container\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = df.loc[row]['Content_Parsed_4']\n",
    "    text_words = text.split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to df\n",
    "df['Content_Parsed_5'] = lemmatized_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/helgy8/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stop words in english\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the list into a new column to be modified directly in panda\n",
    "df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
    "\n",
    "for stop_word in stop_words:\n",
    "# regex replace \n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'High fuel prices hit BA\\'s profits\\n\\nBritish Airways has blamed high fuel prices for a 40% drop in profits.\\n\\nReporting its results for the three months to 31 December 2004, the airline made a pre-tax profit of Â£75m ($141m) compared with Â£125m a year earlier. Rod Eddington, BA\\'s chief executive, said the results were \"respectable\" in a third quarter when fuel costs rose by Â£106m or 47.3%. BA\\'s profits were still better than market expectation of Â£59m, and it expects a rise in full-year revenues.\\n\\nTo help offset the increased price of aviation fuel, BA last year introduced a fuel surcharge for passengers.\\n\\nIn October, it increased this from Â£6 to Â£10 one-way for all long-haul flights, while the short-haul surcharge was raised from Â£2.50 to Â£4 a leg. Yet aviation analyst Mike Powell of Dresdner Kleinwort Wasserstein says BA\\'s estimated annual surcharge revenues - Â£160m - will still be way short of its additional fuel costs - a predicted extra Â£250m. Turnover for the quarter was up 4.3% to Â£1.97bn, further benefiting from a rise in cargo revenue. Looking ahead to its full year results to March 2005, BA warned that yields - average revenues per passenger - were expected to decline as it continues to lower prices in the face of competition from low-cost carriers. However, it said sales would be better than previously forecast. \"For the year to March 2005, the total revenue outlook is slightly better than previous guidance with a 3% to 3.5% improvement anticipated,\" BA chairman Martin Broughton said. BA had previously forecast a 2% to 3% rise in full-year revenue.\\n\\nIt also reported on Friday that passenger numbers rose 8.1% in January. Aviation analyst Nick Van den Brul of BNP Paribas described BA\\'s latest quarterly results as \"pretty modest\". \"It is quite good on the revenue side and it shows the impact of fuel surcharges and a positive cargo development, however, operating margins down and cost impact of fuel are very strong,\" he said. Since the 11 September 2001 attacks in the United States, BA has cut 13,000 jobs as part of a major cost-cutting drive. \"Our focus remains on reducing controllable costs and debt whilst continuing to invest in our products,\" Mr Eddington said. \"For example, we have taken delivery of six Airbus A321 aircraft and next month we will start further improvements to our Club World flat beds.\" BA\\'s shares closed up four pence at 274.5 pence.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking good?\n",
    "df.loc[3]['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'high fuel price hit ba profit  british airways  blame high fuel price   40% drop  profit  report  result   three months  31 december 2004  airline make  pre-tax profit  â£75m ($141m) compare  â£125m  year earlier rod eddington ba chief executive say  result  respectable   third quarter  fuel cost rise  â£106m  473% ba profit  still better  market expectation  â£59m   expect  rise  full-year revenues   help offset  increase price  aviation fuel ba last year introduce  fuel surcharge  passengers   october  increase   â£6  â£10 one-way   long-haul flight   short-haul surcharge  raise  â£250  â£4  leg yet aviation analyst mike powell  dresdner kleinwort wasserstein say ba estimate annual surcharge revenues - â£160m -  still  way short   additional fuel cost -  predict extra â£250m turnover   quarter   43%  â£197bn  benefit   rise  cargo revenue look ahead   full year result  march 2005 ba warn  yield - average revenues per passenger -  expect  decline   continue  lower price   face  competition  low-cost carriers however  say sales would  better  previously forecast   year  march 2005  total revenue outlook  slightly better  previous guidance   3%  35% improvement anticipate ba chairman martin broughton say ba  previously forecast  2%  3% rise  full-year revenue   also report  friday  passenger number rise 81%  january aviation analyst nick van den brul  bnp paribas describe ba latest quarterly result  pretty modest   quite good   revenue side   show  impact  fuel surcharge   positive cargo development however operate margins   cost impact  fuel   strong  say since  11 september 2001 attack   unite state ba  cut 13000 job  part   major cost-cutting drive  focus remain  reduce controllable cost  debt whilst continue  invest   products mr eddington say  example   take delivery  six airbus a321 aircraft  next month   start  improvements   club world flat bed ba share close  four pence  2745 pence'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking good?\n",
    "df.loc[3]['Content_Parsed_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>Complete_Filename</th>\n",
       "      <th>Content_Parsed_1</th>\n",
       "      <th>Content_Parsed_2</th>\n",
       "      <th>Content_Parsed_3</th>\n",
       "      <th>Content_Parsed_4</th>\n",
       "      <th>Content_Parsed_5</th>\n",
       "      <th>Content_Parsed_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "      <td>001.txt-business</td>\n",
       "      <td>Ad sales boost Time Warner profit  Quarterly p...</td>\n",
       "      <td>ad sales boost time warner profit  quarterly p...</td>\n",
       "      <td>ad sales boost time warner profit  quarterly p...</td>\n",
       "      <td>ad sales boost time warner profit  quarterly p...</td>\n",
       "      <td>ad sales boost time warner profit  quarterly p...</td>\n",
       "      <td>ad sales boost time warner profit  quarterly p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File_Name                                            Content  Category  \\\n",
       "0   001.txt  Ad sales boost Time Warner profit\\n\\nQuarterly...  business   \n",
       "\n",
       "  Complete_Filename                                   Content_Parsed_1  \\\n",
       "0  001.txt-business  Ad sales boost Time Warner profit  Quarterly p...   \n",
       "\n",
       "                                    Content_Parsed_2  \\\n",
       "0  ad sales boost time warner profit  quarterly p...   \n",
       "\n",
       "                                    Content_Parsed_3  \\\n",
       "0  ad sales boost time warner profit  quarterly p...   \n",
       "\n",
       "                                    Content_Parsed_4  \\\n",
       "0  ad sales boost time warner profit  quarterly p...   \n",
       "\n",
       "                                    Content_Parsed_5  \\\n",
       "0  ad sales boost time warner profit  quarterly p...   \n",
       "\n",
       "                                    Content_Parsed_6  \n",
       "0  ad sales boost time warner profit  quarterly p...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking good?\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the only columns we want\n",
    "list_columns = [\"File_Name\", \"Category\", \"Complete_Filename\", \"Content\", \"Content_Parsed_6\"]\n",
    "# truncate\n",
    "df = df[list_columns]\n",
    "# get rid of that uncany _6\n",
    "df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new column with these key-values\n",
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4\n",
    "}\n",
    "# Category mapping\n",
    "df['Category_Code'] = df['Category']\n",
    "df = df.replace({'Category_Code':category_codes})\n",
    "# and I recreate the training and test split as before, though I will only test on the hold-out auto articles I found \n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Content_Parsed'], \n",
    "                                                    df['Category_Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Complete_Filename</th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_Parsed</th>\n",
       "      <th>Category_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>001.txt-business</td>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>ad sales boost time warner profit  quarterly p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>002.txt-business</td>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>dollar gain  greenspan speech   dollar  hit  h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>003.txt-business</td>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>yukos unit buyer face loan claim   owners  emb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>004.txt-business</td>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>high fuel price hit ba profit  british airways...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>005.txt-business</td>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>pernod takeover talk lift domecq  share  uk dr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File_Name  Category Complete_Filename  \\\n",
       "0   001.txt  business  001.txt-business   \n",
       "1   002.txt  business  002.txt-business   \n",
       "2   003.txt  business  003.txt-business   \n",
       "3   004.txt  business  004.txt-business   \n",
       "4   005.txt  business  005.txt-business   \n",
       "\n",
       "                                             Content  \\\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...   \n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...   \n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...   \n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...   \n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...   \n",
       "\n",
       "                                      Content_Parsed  Category_Code  \n",
       "0  ad sales boost time warner profit  quarterly p...              0  \n",
       "1  dollar gain  greenspan speech   dollar  hit  h...              0  \n",
       "2  yukos unit buyer face loan claim   owners  emb...              0  \n",
       "3  high fuel price hit ba profit  british airways...              0  \n",
       "4  pernod takeover talk lift domecq  share  uk dr...              0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part I took articles 001, 002 and 003 from my own hunt and put them into the full database, then reran the first part of lesson 3, for speed. I am going to use the now slightly larger database to retrain the model\n",
    "\n",
    "I will then test the model on the last two records, which I will process below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>Complete_Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>405.txt</td>\n",
       "      <td>Tech could help autonomous vehicles read signs...</td>\n",
       "      <td>tech</td>\n",
       "      <td>405.txt-tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>406.txt</td>\n",
       "      <td>Do passengers want self-driving cars to behave...</td>\n",
       "      <td>tech</td>\n",
       "      <td>406.txt-tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File_Name                                            Content Category  \\\n",
       "0   405.txt  Tech could help autonomous vehicles read signs...     tech   \n",
       "1   406.txt  Do passengers want self-driving cars to behave...     tech   \n",
       "\n",
       "  Complete_Filename  \n",
       "0      405.txt-tech  \n",
       "1      406.txt-tech  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the holdout set\n",
    "df_path2 = 'Auto_database_testonly.csv'\n",
    "df2 = pd.read_csv(df_path2, sep=';')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^   It's not very big....\n",
    "\n",
    "Anyway...\n",
    "\n",
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/helgy8/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/helgy8/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This could be a function....\n",
    "\n",
    "df2['Content_Parsed_1'] = df2['Content'].str.replace(\"\\r\", \" \")\n",
    "df2['Content_Parsed_1'] = df2['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "df2['Content_Parsed_1'] = df2['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
    "df2['Content_Parsed_1'] = df2['Content_Parsed_1'].str.replace('\"', '')\n",
    "df2['Content_Parsed_2'] = df2['Content_Parsed_1'].str.lower()\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "df2['Content_Parsed_3'] = df2['Content_Parsed_2']\n",
    "for punct_sign in punctuation_signs:\n",
    "    df2['Content_Parsed_3'] = df2['Content_Parsed_3'].str.replace(punct_sign, '')\n",
    "df2['Content_Parsed_4'] = df2['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nrows = len(df2)\n",
    "lemmatized_text_list = []\n",
    "for row in range(0, nrows):\n",
    "    lemmatized_list = []\n",
    "    text = df2.loc[row]['Content_Parsed_4']\n",
    "    text_words = text.split(\" \")\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    lemmatized_text_list.append(lemmatized_text)\n",
    "df2['Content_Parsed_5'] = lemmatized_text_list\n",
    "stop_words = list(stopwords.words('english'))\n",
    "df2['Content_Parsed_6'] = df2['Content_Parsed_5']\n",
    "for stop_word in stop_words:\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df2['Content_Parsed_6'] = df2['Content_Parsed_6'].str.replace(regex_stopword, '')\n",
    "list_columns = [\"File_Name\", \"Category\", \"Complete_Filename\", \"Content\", \"Content_Parsed_6\"]\n",
    "df2 = df2[list_columns]\n",
    "df2 = df2.rename(columns={'Content_Parsed_6': 'Content_Parsed'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly split up to make sure nothing breaks down\n",
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4\n",
    "}\n",
    "# Category mapping\n",
    "df2['Category_Code'] = df2['Category']\n",
    "df2 = df2.replace({'Category_Code':category_codes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1893, 300)\n",
      "(335, 300)\n",
      "(335, 300)\n"
     ]
    }
   ],
   "source": [
    "# This is a simplification of the process that takes place in box 21 when we split the validation set into the features and labels\n",
    "assignment_x = df['Content_Parsed']\n",
    "assignment_y = df['Category_Code']\n",
    "\n",
    "# Parameter election is the same as in the lesson because the training set is enarly identical\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300\n",
    "\n",
    "# term frequency inverse document freqeuncy \n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "# \"we have fitted and then transformed the training set, but we have only transformed the test set.\"\n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)\n",
    "# and I transform the new set exactly the same way\n",
    "features_assignment = tfidf.transform(assignment_x).toarray()\n",
    "labels_assignment = assignment_y\n",
    "print(features_test.shape)\n",
    "# save everything\n",
    "with open('Pickles/auto_assignment_test.pickle', 'wb') as output:\n",
    "    pickle.dump(df2, output)\n",
    "\n",
    "with open('Pickles/assignment_x.pickle', 'wb') as output:\n",
    "    pickle.dump(assignment_x, output)\n",
    "    \n",
    "with open('Pickles/assignment_y.pickle', 'wb') as output:\n",
    "    pickle.dump(assignment_y, output)\n",
    "    \n",
    "with open('Pickles/X_train.pickle', 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "with open('Pickles/X_test.pickle', 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "with open('Pickles/y_train.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "with open('Pickles/y_test.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "with open('Pickles/df.pickle', 'wb') as output:\n",
    "    pickle.dump(df, output)\n",
    "    \n",
    "# features_train\n",
    "with open('Pickles/features_train.pickle', 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "with open('Pickles/labels_train.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "with open('Pickles/features_test.pickle', 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "with open('Pickles/labels_test.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "    \n",
    "# TF-IDF object\n",
    "with open('Pickles/tfidf.pickle', 'wb') as output:\n",
    "    pickle.dump(tfidf, output)\n",
    "    \n",
    "with open('Pickles/features_assignment.pickle', 'wb') as output:\n",
    "    pickle.dump(features_assignment, output)\n",
    "\n",
    "# labels_test\n",
    "with open('Pickles/labels_assignment.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_assignment, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "path_df = \"Pickles/df.pickle\"\n",
    "with open(path_df, 'rb') as data:\n",
    "    df = pickle.load(data)\n",
    "\n",
    "# features_train\n",
    "path_features_train = \"Pickles/features_train.pickle\"\n",
    "with open(path_features_train, 'rb') as data:\n",
    "    features_train = pickle.load(data)\n",
    "\n",
    "# labels_train\n",
    "path_labels_train = \"Pickles/labels_train.pickle\"\n",
    "with open(path_labels_train, 'rb') as data:\n",
    "    labels_train = pickle.load(data)\n",
    "\n",
    "# features_test is now the assignment set\n",
    "path_features_test = \"Pickles/features_assignment.pickle\"\n",
    "with open(path_features_test, 'rb') as data:\n",
    "    features_test = pickle.load(data)\n",
    "\n",
    "# labels_test is now the assignment set\n",
    "path_labels_test = \"Pickles/labels_assignment.pickle\"\n",
    "with open(path_labels_test, 'rb') as data:\n",
    "    labels_test = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1893, 300)\n",
      "(2228, 300)\n",
      "(1893,)\n",
      "(2228,)\n"
     ]
    }
   ],
   "source": [
    "# size check\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "print(labels_train.shape)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'C': 1.0,\n",
      " 'break_ties': False,\n",
      " 'cache_size': 200,\n",
      " 'class_weight': None,\n",
      " 'coef0': 0.0,\n",
      " 'decision_function_shape': 'ovr',\n",
      " 'degree': 3,\n",
      " 'gamma': 'scale',\n",
      " 'kernel': 'rbf',\n",
      " 'max_iter': -1,\n",
      " 'probability': False,\n",
      " 'random_state': 8,\n",
      " 'shrinking': True,\n",
      " 'tol': 0.001,\n",
      " 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "# load the classifier with the same state as in the lesson\n",
    "svc_0 =svm.SVC(random_state=8)\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(svc_0.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the paramater distributions to startoptimzation over them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': [0.0001, 0.001, 0.01],\n",
      " 'degree': [1, 2, 3, 4, 5],\n",
      " 'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
      " 'kernel': ['linear', 'rbf', 'poly'],\n",
      " 'probability': [True]}\n"
     ]
    }
   ],
   "source": [
    "# C\n",
    "C = [.0001, .001, .01]\n",
    "\n",
    "# gamma\n",
    "gamma = [.0001, .001, .01, .1, 1, 10, 100]\n",
    "\n",
    "# degree\n",
    "degree = [1, 2, 3, 4, 5]\n",
    "\n",
    "# kernel\n",
    "kernel = ['linear', 'rbf', 'poly']\n",
    "\n",
    "# probability\n",
    "probability = [True]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'C': C,\n",
    "              'kernel': kernel,\n",
    "              'gamma': gamma,\n",
    "              'degree': degree,\n",
    "              'probability': probability\n",
    "             }\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:  9.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=SVC(random_state=8), n_iter=50,\n",
       "                   param_distributions={'C': [0.0001, 0.001, 0.01],\n",
       "                                        'degree': [1, 2, 3, 4, 5],\n",
       "                                        'gamma': [0.0001, 0.001, 0.01, 0.1, 1,\n",
       "                                                  10, 100],\n",
       "                                        'kernel': ['linear', 'rbf', 'poly'],\n",
       "                                        'probability': [True]},\n",
       "                   random_state=8, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now create the base model to tune\n",
    "svc = svm.SVC(random_state=8)\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=svc,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search are:\n",
      "{'probability': True, 'kernel': 'poly', 'gamma': 10, 'degree': 4, 'C': 0.01}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.9302694136291602\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  84 out of  84 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=8, test_size=0.33, train_size=None),\n",
       "             estimator=SVC(random_state=8),\n",
       "             param_grid=[{'C': [0.0001, 0.001, 0.01, 0.1], 'kernel': ['linear'],\n",
       "                          'probability': [True]},\n",
       "                         {'C': [0.0001, 0.001, 0.01, 0.1], 'degree': [3, 4, 5],\n",
       "                          'kernel': ['poly'], 'probability': [True]},\n",
       "                         {'C': [0.0001, 0.001, 0.01, 0.1],\n",
       "                          'gamma': [1, 10, 100], 'kernel': ['rbf'],\n",
       "                          'probability': [True]}],\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "C = [.0001, .001, .01, .1]\n",
    "degree = [3, 4, 5]\n",
    "gamma = [1, 10, 100]\n",
    "probability = [True]\n",
    "\n",
    "param_grid = [\n",
    "  {'C': C, 'kernel':['linear'], 'probability':probability},\n",
    "  {'C': C, 'kernel':['poly'], 'degree':degree, 'probability':probability},\n",
    "  {'C': C, 'kernel':['rbf'], 'gamma':gamma, 'probability':probability}\n",
    "]\n",
    "\n",
    "# Create a base model\n",
    "svc = svm.SVC(random_state=8)\n",
    "\n",
    "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
    "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=svc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_sets,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, kernel='linear', probability=True, random_state=8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: \n",
      "0.9619651347068146\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(labels_train, best_svc.predict(features_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: \n",
      "0.9605026929982047\n"
     ]
    }
   ],
   "source": [
    "svc_pred = best_svc.predict(features_test)\n",
    "# Test accuracy\n",
    "print(\"The test accuracy is: \")\n",
    "print(accuracy_score(labels_test, svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       510\n",
      "           1       0.96      0.95      0.96       386\n",
      "           2       0.96      0.95      0.96       417\n",
      "           3       0.97      0.99      0.98       511\n",
      "           4       0.97      0.93      0.95       404\n",
      "\n",
      "    accuracy                           0.96      2228\n",
      "   macro avg       0.96      0.96      0.96      2228\n",
      "weighted avg       0.96      0.96      0.96      2228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(\"Classification report\")\n",
    "print(classification_report(labels_test,svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 921.6x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aux_df = df2[['Category', 'Category_Code']].drop_duplicates().sort_values('Category_Code')\n",
    "conf_matrix = confusion_matrix(labels_test, svc_pred)\n",
    "plt.figure(figsize=(12.8,6))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            xticklabels=aux_df['Category'].values, \n",
    "            yticklabels=aux_df['Category'].values,\n",
    "            cmap=\"Blues\")\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am a little worried the goal of the assignment was to tweak the feature selction algorithms but I feel very strange fitting an svm to just one label, and since you wanted us to get articles about autonomous cars they were all tech."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
